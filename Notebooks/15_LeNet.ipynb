{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e27c646f",
   "metadata": {},
   "source": [
    "# LeNet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57529f96",
   "metadata": {},
   "source": [
    "In this section, we introduce LeNet, one of the earliest convolutional neural networks (CNNs) to gain significant attention for its effectiveness in computer vision. Developed by Yann LeCun during his time at AT&T Bell Labs, LeNet was specifically designed for recognizing handwritten digits (LeCun et al., 1998). The model represented the culmination of nearly a decade of work in CNN research; in fact, LeCun and his collaborators were the first to demonstrate successful training of CNNs using backpropagation (LeCun et al., 1989).\n",
    "\n",
    "When it was introduced, LeNet delivered remarkable results, rivaling the accuracy of support vector machines—the leading supervised learning method of the time, by reaching an error rate below 1% per digit. The architecture was later deployed in practical applications, such as digit recognition for processing ATM deposits, and impressively, some ATMs still rely on the original code developed by Yann LeCun and Leon Bottou in the 1990s."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2912775",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from d2l import torch as d2l"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a340cef9",
   "metadata": {},
   "source": [
    "## Architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a9b75d2",
   "metadata": {},
   "source": [
    "At a broad level, LeNet-5 is organized into two main components: (i) a convolutional encoder made up of two convolutional layers, and (ii) a fully connected block comprising three dense layers.\n",
    "\n",
    "<center>\n",
    "<img src=\"../images/15_Image_1.png\" width=\"800\">\n",
    "\n",
    "Figure 1: Data flow in LeNet. The input is a handwritten digit, the output is a probability over 10 possible outcomes.\n",
    "</center>\n",
    "\n",
    "In LeNet, each convolutional block is composed of a convolutional layer, a sigmoid activation function, and an average pooling step. Unlike modern CNNs, ReLUs and max pooling had not yet been introduced at the time. Both convolutional layers use 5 × 5 kernels to transform inputs into two-dimensional feature maps, increasing the number of channels. The first convolutional layer produces 6 output channels, while the second produces 16. Dimensionality is further reduced through 2 × 2\n",
    "2×2 pooling with stride 2, shrinking the spatial size by a factor of four. The output of the convolutional block thus has the form\n",
    "(batch size, channels, height, width). To connect this output to the dense block, the four-dimensional data must be flattened into a two-dimensional format suitable for fully connected layers, with one dimension indexing batch examples and the other representing the flattened features. The dense block then processes this representation through three fully connected layers containing 120, 84, and finally 10 neurons, where the last layer corresponds to the 10 possible output classes.\n",
    "\n",
    "Although fully grasping the inner workings of LeNet may require some effort, the following code example demonstrates how straightforward it is to implement such models using modern deep learning frameworks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52dabd22",
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_cnn(module):  #@save\n",
    "    \"\"\"Initialize weights for CNNs.\"\"\"\n",
    "    if type(module) == nn.Linear or type(module) == nn.Conv2d:\n",
    "        nn.init.xavier_uniform_(module.weight)\n",
    "\n",
    "class LeNet(d2l.Classifier):  #@save\n",
    "    \"\"\"The LeNet-5 model.\"\"\"\n",
    "    def __init__(self, lr=0.1, num_classes=10):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.LazyConv2d(6, kernel_size=5, padding=2), nn.Sigmoid(),\n",
    "            nn.AvgPool2d(kernel_size=2, stride=2),\n",
    "            nn.LazyConv2d(16, kernel_size=5), nn.Sigmoid(),\n",
    "            nn.AvgPool2d(kernel_size=2, stride=2),\n",
    "            nn.Flatten(),\n",
    "            nn.LazyLinear(120), nn.Sigmoid(),\n",
    "            nn.LazyLinear(84), nn.Sigmoid(),\n",
    "            nn.LazyLinear(num_classes))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fa5680b",
   "metadata": {},
   "source": [
    "Let’s examine the network’s internal operations. By feeding a single-channel (grayscale) image into the model and printing the output shape at each stage, we can verify that the transformations match the structure shown in Fig. 2.\n",
    "\n",
    "<center>\n",
    "<img src=\"../images/15_Image_2.png\" width=\"150\">\n",
    "\n",
    "Figure 2: Compressed notation for LeNet-5.\n",
    "</center>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e7bbf77",
   "metadata": {},
   "outputs": [],
   "source": [
    "@d2l.add_to_class(d2l.Classifier)  #@save\n",
    "def layer_summary(self, X_shape):\n",
    "    X = torch.randn(*X_shape)\n",
    "    for layer in self.net:\n",
    "        X = layer(X)\n",
    "        print(layer.__class__.__name__, 'output shape:\\t', X.shape)\n",
    "\n",
    "model = LeNet()\n",
    "model.layer_summary((1, 1, 28, 28))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3578172f",
   "metadata": {},
   "source": [
    "In LeNet, the spatial dimensions (height and width) shrink as data passes through the convolutional block. The first convolutional layer uses padding to preserve size, while the second does not, reducing both height and width by four pixels. Historically, MNIST images were trimmed from 32 × 32 to 28 × 28 to save storage space. As layers progress, the number of channels grows (from 1 in the input, to 6 after the first convolution, and 16 after the second). Pooling layers further halve the spatial dimensions, and the final fully connected layers reduce everything down to match the number of output classes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ba5571e",
   "metadata": {},
   "source": [
    "## Training\n",
    "\n",
    "After implementing LeNet-5, we test it on the Fashion-MNIST dataset. Although CNNs use fewer parameters than MLPs, they can be more computationally intensive, making GPUs useful for faster training. Using the `d2l.Trainer` class simplifies the process by handling device setup and parameter initialization. As with MLPs, training employs cross-entropy loss minimized through minibatch stochastic gradient descent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aac09597",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = d2l.Trainer(max_epochs=10, num_gpus=1)\n",
    "data = d2l.FashionMNIST(batch_size=128)\n",
    "model = LeNet(lr=0.1)\n",
    "model.apply_init([next(iter(data.get_dataloader(True)))[0]], init_cnn)\n",
    "trainer.fit(model, data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f1aa24c",
   "metadata": {},
   "source": [
    "## Exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57604b8a",
   "metadata": {},
   "source": [
    "1. Let’s modernize LeNet. Implement and test the following changes:\n",
    "- Replace average pooling with max-pooling.\n",
    "- Replace the softmax layer with ReLU.\n",
    "\n",
    "2. Try to change the size of the LeNet style network to improve its accuracy in addition to max-pooling and ReLU.\n",
    "- Adjust the convolution window size.\n",
    "- Adjust the number of output channels.\n",
    "- Adjust the number of convolution layers.\n",
    "- Adjust the number of fully connected layers.\n",
    "- Adjust the learning rates and other training details (e.g., initialization and number of epochs).\n",
    "\n",
    "3. Display the activations of the first and second layer of LeNet for different inputs (e.g., sweaters and coats)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "d2l",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.22"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
