{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "58b6b11a",
   "metadata": {},
   "source": [
    "# Batch Normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "026c0e1b",
   "metadata": {},
   "source": [
    "Training deep neural networks can be challenging, particularly when it comes to achieving convergence within a reasonable timeframe. In this section, we introduce batch normalization, a widely used technique that reliably speeds up the training process (Ioffe and Szegedy, 2015). Batch normalization has enabled practitioners to successfully train networks exceeding 100 layers in depth. An additional advantage of this method is its built-in regularization effect."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62dd94ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from d2l import torch as d2l"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ca14d20",
   "metadata": {},
   "source": [
    "## Implementation from Scratch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1a69a2e",
   "metadata": {},
   "source": [
    "To see how batch normalization works in practice, we implement one from scratch below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bfcb5f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_norm(X, gamma, beta, moving_mean, moving_var, eps, momentum):\n",
    "    # Use is_grad_enabled to determine whether we are in training mode\n",
    "    if not torch.is_grad_enabled():\n",
    "        # In prediction mode, use mean and variance obtained by moving average\n",
    "        X_hat = (X - moving_mean) / torch.sqrt(moving_var + eps)\n",
    "    else:\n",
    "        assert len(X.shape) in (2, 4)\n",
    "        if len(X.shape) == 2:\n",
    "            # When using a fully connected layer, calculate the mean and\n",
    "            # variance on the feature dimension\n",
    "            mean = X.mean(dim=0)\n",
    "            var = ((X - mean) ** 2).mean(dim=0)\n",
    "        else:\n",
    "            # When using a two-dimensional convolutional layer, calculate the\n",
    "            # mean and variance on the channel dimension (axis=1). Here we\n",
    "            # need to maintain the shape of X, so that the broadcasting\n",
    "            # operation can be carried out later\n",
    "            mean = X.mean(dim=(0, 2, 3), keepdim=True)\n",
    "            var = ((X - mean) ** 2).mean(dim=(0, 2, 3), keepdim=True)\n",
    "        # In training mode, the current mean and variance are used\n",
    "        X_hat = (X - mean) / torch.sqrt(var + eps)\n",
    "        # Update the mean and variance using moving average\n",
    "        moving_mean = (1.0 - momentum) * moving_mean + momentum * mean\n",
    "        moving_var = (1.0 - momentum) * moving_var + momentum * var\n",
    "    Y = gamma * X_hat + beta  # Scale and shift\n",
    "    return Y, moving_mean.data, moving_var.data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bde32b03",
   "metadata": {},
   "source": [
    "We can now implement a proper BatchNorm layer, which maintains trainable parameters for scaling (gamma) and shifting (beta), updated during training.\n",
    "\n",
    "The layer also keeps moving averages of means and variances to be used later during inference. Leaving aside the algorithmic details, the key design principle is to separate mathematical operations from implementation overhead. Typically, the batch normalization math is defined in a dedicated function (e.g., `batch_norm`), which is then wrapped into a custom layer. \n",
    "\n",
    "The layer handles tasks like placing data on the correct device, initializing variables, and tracking moving averages. For simplicity, the example requires explicitly specifying the number of features rather than inferring input shapes automatically. In practice, modern deep learning frameworks provide high-level BatchNorm APIs that handle shape detection seamlessly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f54abfd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BatchNorm(nn.Module):\n",
    "    # num_features: the number of outputs for a fully connected layer or the\n",
    "    # number of output channels for a convolutional layer. num_dims: 2 for a\n",
    "    # fully connected layer and 4 for a convolutional layer\n",
    "    def __init__(self, num_features, num_dims):\n",
    "        super().__init__()\n",
    "        if num_dims == 2:\n",
    "            shape = (1, num_features)\n",
    "        else:\n",
    "            shape = (1, num_features, 1, 1)\n",
    "        # The scale parameter and the shift parameter (model parameters) are\n",
    "        # initialized to 1 and 0, respectively\n",
    "        self.gamma = nn.Parameter(torch.ones(shape))\n",
    "        self.beta = nn.Parameter(torch.zeros(shape))\n",
    "        # The variables that are not model parameters are initialized to 0 and\n",
    "        # 1\n",
    "        self.moving_mean = torch.zeros(shape)\n",
    "        self.moving_var = torch.ones(shape)\n",
    "\n",
    "    def forward(self, X):\n",
    "        # If X is not on the main memory, copy moving_mean and moving_var to\n",
    "        # the device where X is located\n",
    "        if self.moving_mean.device != X.device:\n",
    "            self.moving_mean = self.moving_mean.to(X.device)\n",
    "            self.moving_var = self.moving_var.to(X.device)\n",
    "        # Save the updated moving_mean and moving_var\n",
    "        Y, self.moving_mean, self.moving_var = batch_norm(\n",
    "            X, self.gamma, self.beta, self.moving_mean,\n",
    "            self.moving_var, eps=1e-5, momentum=0.1)\n",
    "        return Y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42ce536a",
   "metadata": {},
   "source": [
    "We employed momentum to control how past estimates of the mean and variance are aggregated. Although the term can be misleading, since it is unrelated to the momentum used in optimization, it has become the standard terminology. For consistency with common API conventions, we adopt the same variable name in our implementation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01eeb61f",
   "metadata": {},
   "source": [
    "## LeNet with Batch Normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c77a08e",
   "metadata": {},
   "source": [
    "To illustrate the use of BatchNorm in practice, we incorporate it into a standard LeNet model. Note that batch normalization is placed after convolutional or fully connected layers, but before their associated activation functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d96f4f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BNLeNetScratch(d2l.Classifier):\n",
    "    def __init__(self, lr=0.1, num_classes=10):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.LazyConv2d(6, kernel_size=5), BatchNorm(6, num_dims=4),\n",
    "            nn.Sigmoid(), nn.AvgPool2d(kernel_size=2, stride=2),\n",
    "            nn.LazyConv2d(16, kernel_size=5), BatchNorm(16, num_dims=4),\n",
    "            nn.Sigmoid(), nn.AvgPool2d(kernel_size=2, stride=2),\n",
    "            nn.Flatten(), nn.LazyLinear(120),\n",
    "            BatchNorm(120, num_dims=2), nn.Sigmoid(), nn.LazyLinear(84),\n",
    "            BatchNorm(84, num_dims=2), nn.Sigmoid(),\n",
    "            nn.LazyLinear(num_classes))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d822aff0",
   "metadata": {},
   "source": [
    "As before, we will train the network using the Fashion-MNIST dataset. The code is nearly the same as the implementation used earlier for training LeNet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "254b13b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = d2l.Trainer(max_epochs=10, num_gpus=1)\n",
    "data = d2l.FashionMNIST(batch_size=128)\n",
    "model = BNLeNetScratch(lr=0.1)\n",
    "model.apply_init([next(iter(data.get_dataloader(True)))[0]], d2l.init_cnn)\n",
    "trainer.fit(model, data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77a8b932",
   "metadata": {},
   "source": [
    "Letâ€™s examine the learned values of the scale parameter (gamma) and the shift parameter (beta) from the first batch normalization layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba325357",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.net[1].gamma.reshape((-1,)), model.net[1].beta.reshape((-1,))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71b79f0c",
   "metadata": {},
   "source": [
    "## Concise Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df0349b0",
   "metadata": {},
   "source": [
    "Instead of using the custom BatchNorm class we implemented, we can directly rely on the BatchNorm class provided by high-level deep learning APIs. The code remains almost the same, but with the advantage that we no longer need to manually specify arguments for handling the dimensions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae4f48e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BNLeNet(d2l.Classifier):\n",
    "    def __init__(self, lr=0.1, num_classes=10):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.LazyConv2d(6, kernel_size=5), nn.LazyBatchNorm2d(),\n",
    "            nn.Sigmoid(), nn.AvgPool2d(kernel_size=2, stride=2),\n",
    "            nn.LazyConv2d(16, kernel_size=5), nn.LazyBatchNorm2d(),\n",
    "            nn.Sigmoid(), nn.AvgPool2d(kernel_size=2, stride=2),\n",
    "            nn.Flatten(), nn.LazyLinear(120), nn.LazyBatchNorm1d(),\n",
    "            nn.Sigmoid(), nn.LazyLinear(84), nn.LazyBatchNorm1d(),\n",
    "            nn.Sigmoid(), nn.LazyLinear(num_classes))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "884adfd5",
   "metadata": {},
   "source": [
    "Next, we train the model using the same hyperparameters. As expected, the high-level API version executes significantly faster, since its operations are compiled into C++ or CUDA, whereas our custom implementation runs through Python interpretation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00959010",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = d2l.Trainer(max_epochs=10, num_gpus=1)\n",
    "data = d2l.FashionMNIST(batch_size=128)\n",
    "model = BNLeNet(lr=0.1)\n",
    "model.apply_init([next(iter(data.get_dataloader(True)))[0]], d2l.init_cnn)\n",
    "trainer.fit(model, data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74d8172f",
   "metadata": {},
   "source": [
    "On a more practical note, there are a number of aspects worth remembering about batch\n",
    "normalization: \n",
    "- During model training, batch normalization continuously adjusts the intermediate output of the network by utilizing the mean and standard deviation of the minibatch, so that the values of the intermediate output in each layer throughout the neural network are\n",
    "more stable. \n",
    "- Batch normalization is slightly different for fully connected layers than for convolutional layers. In fact, for convolutional layers, layer normalization can sometimes be used as\n",
    "an alternative. \n",
    "- Like a dropout layer, batch normalization layers have different behaviors in training mode\n",
    "than in prediction mode. \n",
    "- Batch normalization is useful for regularization and improving convergence in optimizaion. By contrast, the original motivation of reducing internal covariate shift seems\n",
    "not to be a valid explanation. \n",
    "- For more robust models that are less sensitive to input perturbations, consider removing\n",
    "batch normalization (Wang et al., 2022)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5489f9d",
   "metadata": {},
   "source": [
    "## Exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c237aa15",
   "metadata": {},
   "source": [
    "1. Compare the learning rates for LeNet with and without batch normalization.\n",
    "- Plot the increase in validation accuracy.\n",
    "- How large can you make the learning rate before the optimization fails in both cases?\n",
    "\n",
    "2. Do we need batch normalization in every layer? Experiment with it.\n",
    "\n",
    "3. Can you replace dropout by batch normalization? How does the behavior change?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "d2l",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.22"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
